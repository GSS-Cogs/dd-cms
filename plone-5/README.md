# Plone

Before running Plone, you may need to install some [Prerequisites](./plone.md#1-pre-requisites). 
The following guidance assumes working versions of docker and access to GCP for fetching SQL backups.
You may also need to run some manual steps to [configure plone](./plone.md#configure-plone) once your containers are running.

Plone requires the following env vars to run and connect to the postgres backend.

| Env var | Description | Deployed Value |
| --- | ---- | --- |
| POSTGRES_DB | The database name used in postgres | Same as environment, i.e sandbox/staging/production |
| POSTGRES_USER | The user plone connect to postgres with | `plone`|
| POSTGRES_PASSWORD | The password plone connect to postgres with | A large random string generated by terraform IAC |
| POSTGRES_HOST | The IP address of the database | An internal GCP IP address in the appropriate VPC. |

Within a deployment to environment buld pipleine everything bar `POSTGRES_USER` needs to be set using a docker `--buld-arg`.

## Running plone locally with PostgresDB

When run in an environment Plone uses a postgres database hosted on cloudsql.

To create this locally, you need to...

### 1. Run postgres locally

Run postgres with docker (pick a suitable `<password>` for your own local development work).

```
docker run -e POSTGRES_USER=plone -e POSTGRES_DB=local -e POSTGRES_PASSWORD=<password> --name plone-postgres -d -p 5432:5432 postgres:14
```

notes:
- the `--name` here is just the name of the docker container we're creating, it has nothing to do with the DB connection.
- its `postgres:14` as that's what CloudSQL is using at time of writing, you can check this via the CloudSQL page in the GCP console.


### 2. Import a representative data backup from GCP

Download the desired backup from the google bucket `cms-sql-backup`. Sandbox or staging backups are preferred for local running. Using production backups should only occur with Service Manager consent, as there are security risks involved.

Import the SQL into your locally running postgres instance via:

```
docker exec -i plone-postgres psql -U plone -d local < <PATH_TO_DOWNLOADED_SQL_DUMP>.sql
```

_Note - you will get warnings for missing "cloudql-*" roles but it still works._

### 3. Run plone locally with docker

Build the image (from within the /plone-5 directory).

```
# Note the "." means "current context, i.e ./*", so instructs docker to use
# the Dockerfile in the current working directory - plone-5.
docker build --tag plone .  
```

run it

```
docker run -e POSTGRES_PASSWORD=<password> -p 8080:8080 plone
```

**Important** - there's no oauth handler set up for localhost, you'll need to log into your zope user (for whatever env you copied the database from) to bypass this broken handshake.

Any issues, you can sanity check what's been set in the `zope.conf` (by the dockerfile) via:

```
docker run -e POSTGRES_PASSWORD=<password> -p 8080:8080 plone

# use docker ps to get the container ID
docker ps

# then check the content of zope.conf
docker exec <CONTAINER ID> cat /plone/instance/parts/instance/etc/zope.conf

# NOTE - do NOT try and combine the commands with the -it flag, you'll get the
# zope.conf without the env var injection.
```


## Additional Info

To connect to a local plone instance (docker or otherwise) with volto, from `../volto` run:

```
RAZZLE_DEV_PROXY_API_PATH=http://localhost:8080/<site> yarn start:dev
```

for example

```
RAZZLE_DEV_PROXY_API_PATH=http://localhost:8080/climate-change yarn start:dev
```

## Using postgres with docker-compose

**N.B. Run these command in the same directory as the `docker-compose.yml` file.**

Create the `plone-postgres-data` directory to store the postgres data between runs:

```bash
mkdir plone-postgres-data
```

We need to build the local postgres & plone docker images before we do anything:

```bash
docker-compose build
```

You should also run this any time you need to update the plone docker image.

Next, start postgres by itself:

```bash
docker-compose up postgres
```

Now, in a fresh terminal, restore your backup taken from the `cms-sql-backup` GCP bucket:

(make sure to replace the file name with the filename of your backup)

```bash
docker exec -i plone-postgres psql -U plone -d local < ~/Downloads/staging_staging_2023-01-24.sql
```

Now send postgres the kill signal so it knows to politely write all necessary files to disk:

```bash
docker exec -i plone-postgres bash -c "kill -INT \$(head -1 /var/lib/postgresql/data/postmaster.pid)"
```

This should result in the initial docker-compose command terminating once postgres has safely shutdown.

Now you can start the whole docker-compose operation up and wait for your services to be ready to accept connections:

```bash
docker-compose up
```

Once everything's up and running, give yourself a new Zope user:

```bash
docker exec -i dd-cms-backend /plone/instance/bin/instance adduser <username> <password>
```

(Make sure to stick your proposed username and password in there).

Once you're able to log in to Zope, you'll be able to apply any database upgrades that are necessary due to the difference in versions between the database copy that you applied and the version of plone which is currently running.
